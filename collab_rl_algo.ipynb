{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/Tzbc1k9dFRxQ8oDhOZF9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RatishMoondra/MS_Thesis/blob/main/collab_rl_algo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "rPTK5vdw9yT0",
        "outputId": "7384bf0a-5ffb-4ac7-e8bc-2060cc653717"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OperationalError",
          "evalue": "no such table: new_prices",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e934ffb67b54>\u001b[0m in \u001b[0;36m<cell line: 136>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;31m# Fetch data from database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"SELECT Close FROM new_prices WHERE StockName='{ticker}' \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperationalError\u001b[0m: no such table: new_prices"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "import random\n",
        "from collections import deque\n",
        "import sqlite3\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow logging (1: INFO, 2: WARNING, 3: ERROR)\n",
        "\n",
        "\n",
        "class StockTradingEnvironment:\n",
        "    def __init__(self, df, window_size=14, initial_balance=1000):\n",
        "        self.df = df\n",
        "        self.window_size = window_size\n",
        "        self.initial_balance = initial_balance\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.balance = self.initial_balance\n",
        "        self.shares_owned = 0\n",
        "        self.net_worth = self.balance\n",
        "        self.current_step = self.window_size  # Start from window size to have enough history\n",
        "        self.max_steps = len(self.df) - 1\n",
        "        self.state = self.get_state()\n",
        "        return self.state\n",
        "\n",
        "    def get_state(self):\n",
        "        start_idx = max(0, self.current_step - self.window_size)\n",
        "        end_idx = self.current_step\n",
        "        state = self.df['Close'].values[start_idx:end_idx]\n",
        "        return np.reshape(state, (self.window_size, 1))\n",
        "\n",
        "    def step(self, action):\n",
        "        self.current_step += 1\n",
        "        reward = 0\n",
        "        print(f\"(PRE) Current Step : {self.current_step} Action: {action} Balance {self.balance} Share Ownned {self.shares_owned} Close price {self.df['Close'][self.current_step]}\")\n",
        "        if action == 1:\n",
        "            if self.balance >= self.df['Close'][self.current_step]:\n",
        "                self.shares_owned += 1\n",
        "                self.balance -= self.df['Close'][self.current_step]\n",
        "            else:\n",
        "                print(f\"*****BALANCE<CLOSEPRICE\")\n",
        "                reward -= 5\n",
        "        elif action == 2:\n",
        "            if self.shares_owned > 0:\n",
        "                self.shares_owned -= 1\n",
        "                self.balance += self.df['Close'][self.current_step]\n",
        "            else:\n",
        "                print(f\"*****NO SHARWS OWNED BUT SELLING****\")\n",
        "                reward = -5\n",
        "\n",
        "        self.net_worth = self.balance + self.shares_owned * self.df['Close'][self.current_step]\n",
        "        reward += (self.net_worth - self.initial_balance) / self.initial_balance\n",
        "\n",
        "        print(f\"(POST) Balance {self.balance} Share Ownned {self.shares_owned} Reward {reward} Net Worth {self.net_worth}\")\n",
        "\n",
        "        next_state = self.get_state()\n",
        "\n",
        "        done = self.current_step == self.max_steps\n",
        "        print(f\"Initial Balance {self.initial_balance} / Balance {self.balance}/ Net_worth {self.net_worth} / Reward = {reward} \")\n",
        "        return next_state, reward, done\n",
        "\n",
        "class RLAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = Sequential([\n",
        "            LSTM(50, input_shape=(self.state_size[0], self.state_size[1])),\n",
        "            Dense(25, activation='relu'),\n",
        "            Dense(self.action_size, activation='linear')\n",
        "        ])\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "\n",
        "        return np.argmax(self.model.predict(np.array([state]), verbose=0)[0])\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = reward + self.gamma * np.amax(self.model.predict(np.array([next_state]), verbose=0)[0])\n",
        "            target_f = self.model.predict(np.array([state]), verbose=0)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(np.array([state]), target_f, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def train(self, env, episodes, batch_size):\n",
        "        for episode in range(episodes):\n",
        "            print(f\"Processing Episode# {episode}\")\n",
        "            state = env.reset()\n",
        "            for step in range(env.max_steps):\n",
        "                action = self.act(state)\n",
        "                if action == 0:\n",
        "                    agent_action = \"hold\"\n",
        "                elif action == 1:\n",
        "                    agent_action = \"buy\"\n",
        "                elif action == 2:\n",
        "                    agent_action = \"sell\"\n",
        "\n",
        "                print(f\"Processing Episode {episode} Step Number {step} Action {action} Agent Action {agent_action}\")\n",
        "\n",
        "\n",
        "                next_state, reward, done = env.step(action)\n",
        "                self.remember(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                if done:\n",
        "                    print(f\"Episode: {episode+1}/{episodes}, Reward: {reward:.2f}, Epsilon: {self.epsilon:.2f}\")\n",
        "                    break\n",
        "                if len(self.memory) > batch_size:\n",
        "                    self.replay(batch_size)\n",
        "\n",
        "# Connect to SQLite database\n",
        "conn = sqlite3.connect('stock_data.db')\n",
        "ticker='MSFT'\n",
        "# Fetch data from database\n",
        "cursor = conn.cursor()\n",
        "cursor.execute(f\"SELECT Close FROM new_prices WHERE StockName='{ticker}' \")\n",
        "data = cursor.fetchall()\n",
        "conn.close()\n",
        "\n",
        "# Convert the data into a pandas DataFrame\n",
        "df = pd.DataFrame(data, columns=['Close'])\n",
        "\n",
        "# # Normalize the data\n",
        "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "# scaled_data = scaler.fit_transform(df[['Close']])\n",
        "# df['Close'] = scaled_data\n",
        "\n",
        "# Initialize environment, agent, and train the agent\n",
        "env = StockTradingEnvironment(df)\n",
        "state_size = (env.window_size, 1)  # Make sure state_size is a tuple\n",
        "action_size = 3  # 3 actions: hold, buy, sell\n",
        "agent = RLAgent(state_size, action_size)\n",
        "agent.train(env, episodes=1, batch_size=32)\n"
      ]
    }
  ]
}